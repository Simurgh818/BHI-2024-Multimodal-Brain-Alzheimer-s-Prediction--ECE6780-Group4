\section{Discussion: Implications of Multimodal Approaches for Early Alzheimerâ€™s Prediction.}
\label{sec:discussion}
\subsection{Multimodal Alzheimer's Disease Classification}
A key innovation of our approach is to exploit complementary information from CNN and ViT. CNN is good at capturing local features, while ViT is good at modeling global dependencies. By combining these two types of features, our model can represent brain imaging data more comprehensively, thereby improving the performance of AD classification. Furthermore, by leveraging pre-trained ViT models, our approach can effectively leverage universal visual representations learned on large-scale datasets. 

We found that the hybrid ViT-CNN model showed better consistency, and its validation results were closer to the test results (validation accuracy 77\%, test accuracy 70\%). In contrast, the performance of the CNN encoder model on the validation set and test set differs greatly. This shows that the hybrid ViT-CNN model has stable generalization ability. 

It is worth noting that the ViT model alone has the lowest performance. We believe this may be because the pre-trained ViT model is trained on a large-scale natural image dataset, and medical imaging data is significantly different from natural images. 

By incorporating a pretrained ViT alongside the CNN encoder, the hybrid model achieves a remarkable increase in sensitivity from approximately 52\% to around 80\%, while maintaining consistent hyperparameters. This superior performance can be attributed to the pretrained ViT's ability to facilitate effective multi-modality integration and the concatenation of diverse features extracted from both the CNN and ViT. The promising results suggest that the hybrid ViT-CNN model has the potential to become a state-of-the-art approach for AD classification. Visualization techniques such as t-SNE and Grad-Cam will further elucidate the important regions learned by the model, contributing to the development of more accurate and reliable AD diagnosis techniques. 

\subsection{Multimodal Alzheimer's Disease Progression}
The hyperparameter optimization results suggest that dimensionality reduction using PCA does not help the classification performance. The SVM with 50 components outperforms 1 and 10 components. In addition, although for 3 out of 7 timepoints ViT had higher Test Sensitivity scores than CNN, for timepoints 3-5 they were similar and for timepoint 6 CNN had higher Test Sensitivity scores. Overall, the test sensitivity scores were lower than expected. This could be since these models were pretrained on non-medical images. It is essential to do transfer learning and train them on medical images before doing feature extraction. 

The hybrid model, ResNet18-ViT, did go through transfer learning. One model got trained on MRI images from all time points for MRI feature extraction and a separate model got transfer learning training on PET images from all time points for PET feature extraction. Even though, theoretically the hybrid model should outperform the CNN and ViT models, but the test sensitivity scores were below 22 \% for the seven timepoints and training sensitivity got obove 50 \% for three out of seven timepoints. Therefore, for this dataset the ViT model outperformed the CNN and the hybrid model performed worse on the sensitivity scores. 

\subsection{Analysis of Multimodal Contributions to AD Classification and Progression Prediction.} 

\textbf{AD Classification: }

In the single modality setting, PET demonstrated superior performance compared to MRI. The accuracy of PET reached 59\%, surpassing that of MRI at 55\%. Moreover, PET exhibited higher F1 score and sensitivity, further highlighting its diagnostic value in AD classification. These findings underscore the importance of PET imaging in capturing AD-related pathological changes. However, in spirit of transparency the MRI images in the dataset we used were masks and not true MRI images. We are re-running our analysis with a hybrid dataset that combines good MRI images with PET images and will report our updated results in future.  

Moving on to multimodal modeling, the combination of MRI and PET improved significantly. The accuracy of the MRI and PET combined model reached 63\%, outperforming the individual modalities. The F1 score and sensitivity also witnessed similar enhancements. This suggests that the complementary information provided by MRI and PET contributes to a more comprehensive representation of AD characteristics, leading to improved classification performance. 

Furthermore, by comparing the results across different hyperparameter settings, we observed that multimodal models exhibited greater stability. Even with the inclusion of EHR and genomic data, the performance remained comparable to the MRI and PET combination, indicating the robustness of multimodal models to data variations and noise. 

Regarding the limited improvement brought by the addition of EHR and genomic data, the analysis points to the limitation on the number of feature types  as a potential factor. Acquiring large-scale EHR and genomic data poses greater challenges compared to imaging data. Therefore, future work should focus on collecting more comprehensive EHR and genomic samples to fully harness their potential in AD classification. 

In conclusion, the modality analysis conducted through testing results corroborates our previous observations. PET excels in single modality performance, while multimodal modeling significantly enhances the accuracy and stability of AD classification. Although the contribution of EHR and genomic data is limited in the current study, it highlights the importance of gathering more extensive and high-quality multimodal data. By integrating imaging, clinical, and genomic information, we can strive towards building more precise and robust models for AD diagnosis. 

As shown in Figure 6, we also investigated the contribution of different modalities to Alzheimer's Disease (AD) classification, we conducted feature permutation experiments in the multimodal ViT-CNN model. Our analysis showed that features of PET in the CNN component had the highest importance, corroborating earlier findings that PET is effective in AD diagnosis. MRI and PET features within the CNN and ViT components also made substantial contributions, particularly when combined, highlighting the model's ability to utilize complementary information from both modalities to enhance classification performance. 

In contrast, EHR features contributed the least, likely due to the limited sample size in our dataset, which may not fully represent the potential of EHR in AD diagnosis. This emphasizes the need for larger EHR datasets to better explore its utility. 

The bar graph illustrating modality contributions clearly shows PET as the most significant contributor, followed by combined PET \& MRI features. The findings validate our hypothesis that multimodal approaches improve classification performance and stability. 

Future research should aim to expand EHR data collection and explore methods to better integrate it with imaging modalities to enhance the performance of multimodal models. Overall, these insights confirm the importance of multimodal modeling in AD diagnosis and underscore the need for comprehensive datasets that include imaging, clinical, and genomic information. 

\textbf{AD Progression: }

The contribution of modalities and their performance metrics of accuracy, F1 score, and Test Sensitivity oscillated across timepoints and the range of values for CNN, ViT and ResNet18-ViT were similar. One strange observation is that the F1 score and Test Sensitivity for EHR and Genetic features were zero for timepoint 3 to 6 regardless of the model used (\autoref{fig:ADprogression}). The EHR consisted of binary biological sex and normalized age variable and the genetic feature is the normalized polygentic hazard score (PHS). Since the sex and and PHS does not change across time points, these features would not theoretically be useful in predicting conversion from sMCI to pMCI. However, age should be a predictive feature. The age variable has a mean of 77.2, min of 55, 25\% of 73, median of 77, 75\% of 82 and max of 96. One possibility could be that timepoints 3-6 lies in the 73-82 years of age and most patients are already pMCI and there is not a lot of switches between sMCI to pMCI. 

\