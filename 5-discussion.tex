\section{Discussion: Implications of Multimodal Approaches for Early Alzheimer’s Prediction.}
\label{sec:discussion}
\subsection{Multimodal Alzheimer's Disease Classification}
Our results demonstrate that the hybrid ViT-CNN model exhibits stronger consistency and generalization compared to standalone CNN or ViT models. The hybrid model achieved 77\% validation accuracy and 70\% test accuracy, showing stable performance across datasets. In contrast, the CNN encoder alone showed a notable difference between validation and test results, while the ViT model had the lowest performance. This is likely due to the fact that the pre-trained ViT was trained on natural images, which differ significantly from medical imaging data.

The incorporation of a pretrained ViT alongside the CNN encoder significantly improved sensitivity from approximately 52\% to 80\%, without requiring major adjustments in hyperparameters. This improvement highlights the strength of combining diverse feature extraction methods from both CNN and ViT, resulting in a comprehensive representation of AD characteristics. The promising performance of the hybrid model suggests that it has the potential to become a leading approach in AD classification. In future work, techniques such as t-SNE and Grad-Cam could provide further insights into the regions of importance learned by the model, which could contribute to the development of more accurate and interpretable diagnostic tools.

\subsection{Multimodal Alzheimer's Disease Progression}
Our experiments on AD progression prediction revealed that using dimensionality reduction via PCA did not enhance classification performance. In fact, the SVM classifier performed best with 50 components compared to 1 or 10 components. Sensitivity scores for the ViT model exceeded those of CNN in 3 out of 7 timepoints, but overall, sensitivity was lower than anticipated. This may be due to the pretraining of models on non-medical images, emphasizing the need for transfer learning on medical imaging data before feature extraction.

Interestingly, the hybrid ResNet18-ViT model, which underwent transfer learning on MRI and PET images from all timepoints, performed worse than expected, particularly in sensitivity scores, where it achieved below 22\% for the seven timepoints. In contrast, the ViT model consistently outperformed both CNN and the hybrid model in sensitivity across timepoints. This highlights a key limitation in the current dataset and indicates a need for further refinement of the hybrid approach to achieve better results in progression prediction.

We have explored several hyperparameter configurations to enhance the model's performance in predicting AD progression. These include: SVM with 1, 10, and 50 components, implementing weight decay values of  $1 \times 10^{-3}$ and $1 \times 10^{-4}$, applying data augmentation techniques such as vertical flip, random resize crop, and brightness/contrast adjustment, incorporating a dropout rate of 50\% and early stopping with a patience of 5 epochs, conducting cross-validation with folds of 3, 5, and 10, early layer freezing of ResNet18 and Vision Transformers (ViT), implementing label smoothing. Despite these efforts, the hybrid model has shown inconsistent performance in progression tasks. 

\subsection{Analysis of Multimodal Contributions to AD Classification and Progression Prediction.} 

\textbf{AD Classification:} Among individual modalities, PET outperformed MRI, achieving an accuracy of 59\% compared to 55\%. PET also had higher F1 scores and sensitivity, demonstrating its strength in capturing AD-related changes. However, it is important to note that the MRI images in our dataset were masks rather than raw MRI scans, which may have influenced the results. We plan to rerun the analysis with a hybrid dataset that includes true MRI images alongside PET data.

When MRI and PET were combined, classification performance improved substantially, with accuracy increasing to 63\%. The F1 score and sensitivity also benefited from the multimodal approach, indicating that the complementary information provided by these imaging modalities enhances the model’s ability to capture AD characteristics. Multimodal models also exhibited greater stability across different hyperparameter settings, maintaining performance even when Electronic Health Records (EHR) and genomic data were included, though these additions provided limited improvement. This suggests that the integration of larger and more diverse EHR and genomic datasets is necessary to fully realize their potential in AD classification.

Our modality analysis aligns with these findings: PET was the most significant contributor to the hybrid ViT-CNN model’s performance, while the combination of MRI and PET proved beneficial. EHR features, on the other hand, contributed the least, likely due to the small sample size and limited representation in our dataset. Future research should prioritize expanding EHR data collection to explore its full utility.

\textbf{AD Progression:} In terms of predicting AD progression, the contribution of modalities varied across timepoints. Accuracy, F1 score, and sensitivity oscillated, but CNN, ViT, and ResNet18-ViT performed similarly overall. Notably, EHR and Genetic features showed zero F1 scores and sensitivity for timepoints 3 through 6, likely because these features (biological sex, age, and Polygenic Hazard Score) do not change across timepoints, limiting their predictive value for progression. While age should theoretically be a predictive feature, it is possible that during timepoints 3-6, most patients had already progressed from sMCI to pMCI, leading to fewer detectable transitions.

\subsection{Summary and Future Directions}

In summary, our study demonstrates the efficacy of a hybrid ViT-CNN model for Alzheimer’s disease classification, leveraging the complementary strengths of CNN and ViT to capture both local and global features from multimodal data. PET emerged as the most valuable single modality, while combining MRI and PET significantly improved classification accuracy and stability. However, the limited contribution of EHR and genomic data emphasizes the need for larger and more diverse datasets to fully harness the power of multimodal approaches.

For AD progression, our findings indicate that further refinement of the hybrid model and more extensive transfer learning on medical imaging data are necessary to improve sensitivity. The analysis also highlights the variability in multimodal performance across timepoints, suggesting that different data integration strategies may be required for progression prediction.

Future work should focus on expanding and improving multimodal datasets, particularly EHR and genomic data, and exploring advanced techniques to enhance feature extraction and data integration. By continuing to refine these models, we can move closer to more accurate, stable, and interpretable methods for early Alzheimer’s diagnosis and progression prediction.